# Crawl4AI 进阶中文示例

这个文档介绍了 Crawl4AI 的高级功能和使用技巧。

## 示例分类

### 1. 钩子函数 (Hooks)
**文件**: `hooks_example.py`

钩子函数允许你在爬虫的不同阶段插入自定义代码：
- `on_browser_created`: 浏览器创建后
- `on_page_context_created`: 页面上下文创建后
- `before_goto`: 导航前
- `after_goto`: 导航后
- `before_retrieve_html`: 获取 HTML 前
- `before_return_html`: 返回 HTML 前

**使用场景**:
- 设置认证信息
- 修改请求头
- 等待特定元素加载
- 自定义页面交互

### 2. 浏览器优化
**文件**: `browser_optimization_example.py`

展示如何优化浏览器性能：
- 会话重用以减少资源消耗
- 并行爬取提高效率
- 性能监控和分析
- 资源使用优化

**优化技巧**:
- 使用无头模式
- 禁用 GPU 加速
- 设置合适的视口大小
- 控制并发数量

### 3. 数据提取策略

#### CSS 选择器提取
```python
# 使用 CSS 选择器提取特定内容
crawler_config = CrawlerRunConfig(
    css_selector=".product-title"  # 提取产品标题
)
```

#### JavaScript 执行
```python
# 执行 JavaScript 代码
js_code = """
document.querySelector('button.load-more').click();
"""
crawler_config = CrawlerRunConfig(js_code=js_code)
```

#### LLM 提取
```python
# 使用大语言模型提取结构化数据
class ProductInfo(BaseModel):
    name: str = Field(description="产品名称")
    price: str = Field(description="产品价格")

strategy = LLMExtractionStrategy(
    provider="openai",
    api_token="your-api-key",
    schema=ProductInfo
)
```

### 4. 内容处理

#### 内容过滤
```python
# 过滤无关内容
content_filter = PruningContentFilter(
    threshold=0.48,  # 相关性阈值
    threshold_type="fixed"
)
```

#### Markdown 生成
```python
# 生成 Markdown 格式内容
markdown_generator = DefaultMarkdownGenerator(
    content_filter=content_filter,
    options={"ignore_links": True}  # 忽略链接
)
```

### 5. 缓存和性能

#### 缓存模式
```python
# 缓存模式选项
CacheMode.BYPASS     # 绕过缓存
CacheMode.ENABLED    # 启用缓存
CacheMode.WRITE_ONLY # 仅写入缓存
CacheMode.READ_ONLY  # 仅读取缓存
```

#### 并发控制
```python
# 控制并发数量
max_concurrent = 3  # 最大并发数
tasks = []
for url in urls:
    task = crawler.arun(url=url, config=config)
    tasks.append(task)
results = await asyncio.gather(*tasks)
```

## 最佳实践

### 1. 错误处理
```python
try:
    result = await crawler.arun(url=url, config=config)
    if result.success:
        # 处理成功结果
        pass
    else:
        # 处理失败情况
        print(f"爬取失败: {result.error_message}")
except Exception as e:
    # 处理异常情况
    print(f"发生异常: {str(e)}")
```

### 2. 资源管理
```python
# 使用上下文管理器确保资源正确释放
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url=url, config=config)
    # 爬虫实例会自动关闭
```

### 3. 会话重用
```python
# 重用会话提高效率
session_id = "my_session"
result1 = await crawler.arun(url=url1, config=config, session_id=session_id)
result2 = await crawler.arun(url=url2, config=config, session_id=session_id)
```

### 4. 代理使用
```python
# 配置代理
browser_config = BrowserConfig(
    proxy_config={
        "server": "http://proxy.example.com:8080",
        "username": "username",
        "password": "password"
    }
)
```

## 常见问题解决

### 1. 页面加载慢
- 增加超时时间
- 使用 `wait_for` 参数等待特定元素
- 禁用图片加载提高速度

### 2. 被反爬虫拦截
- 使用代理 IP
- 设置合理的 User-Agent
- 控制爬取频率
- 使用无头浏览器

### 3. 内存使用过高
- 及时关闭爬虫实例
- 控制并发数量
- 使用内容过滤减少数据量

### 4. JavaScript 渲染问题
- 确保启用 JavaScript
- 使用 `wait_for` 等待动态内容加载
- 执行自定义 JavaScript 代码

## 性能优化建议

1. **重用浏览器实例**: 避免频繁创建和销毁浏览器
2. **合理设置并发**: 根据系统资源设置合适的并发数
3. **使用缓存**: 对重复请求的页面启用缓存
4. **内容过滤**: 只提取需要的内容，减少数据处理量
5. **代理轮换**: 使用多个代理分散请求负载
6. **错误重试**: 实现重试机制处理临时失败

## 下一步学习

- 查看 `adaptive_crawling/` 文件夹了解自适应爬虫
- 研究 `extraction_strategies/` 学习数据提取策略
- 探索 `docker/` 文件夹了解部署选项
- 阅读官方文档获取最新功能信息