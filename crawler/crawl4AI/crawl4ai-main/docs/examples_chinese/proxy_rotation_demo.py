import os
import re
from typing import List, Dict
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
    RoundRobinProxyStrategy
)

def load_proxies_from_env() -> List[Dict]:
    """ä»PROXIESç¯å¢ƒå˜é‡åŠ è½½ä»£ç†"""
    proxies = []
    try:
        proxy_list = os.getenv("PROXIES", "").split(",")
        for proxy in proxy_list:
            if not proxy:
                continue
            ip, port, username, password = proxy.split(":")
            proxies.append({
                "server": f"http://{ip}:{port}",
                "username": username,
                "password": password,
                "ip": ip  # å­˜å‚¨åŸå§‹IPç”¨äºéªŒè¯
            })
    except Exception as e:
        print(f"ä»ç¯å¢ƒåŠ è½½ä»£ç†æ—¶å‡ºé”™: {e}")
    return proxies

async def demo_proxy_rotation():
    """
    ä½¿ç”¨RoundRobinProxyStrategyçš„ä»£ç†è½®æ¢æ¼”ç¤º
    ===============================================
    æ¼”ç¤ºä½¿ç”¨ç­–ç•¥æ¨¡å¼çš„ä»£ç†è½®æ¢ã€‚
    """
    print("\n=== ä»£ç†è½®æ¢æ¼”ç¤ºï¼ˆè½®è¯¢ï¼‰ ===")
    
    # åŠ è½½ä»£ç†å¹¶åˆ›å»ºè½®æ¢ç­–ç•¥
    proxies = load_proxies_from_env()
    if not proxies:
        print("ç¯å¢ƒä¸­æœªæ‰¾åˆ°ä»£ç†ã€‚è®¾ç½®PROXIESç¯å¢ƒå˜é‡ï¼")
        return
        
    proxy_strategy = RoundRobinProxyStrategy(proxies)
    
    # åˆ›å»ºé…ç½®
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        proxy_rotation_strategy=proxy_strategy
    )
    
    # æµ‹è¯•URL
    urls = ["https://httpbin.org/ip"] * len(proxies)  # æ¯ä¸ªä»£ç†æµ‹è¯•ä¸€æ¬¡
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for url in urls:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success:
                # ä»å“åº”ä¸­æå–IP
                ip_match = re.search(r'(?:[0-9]{1,3}\.){3}[0-9]{1,3}', result.html)
                current_proxy = run_config.proxy_config if run_config.proxy_config else None
                
                if current_proxy:
                    print(f"ä»£ç† {current_proxy['server']} -> å“åº”IP: {ip_match.group(0) if ip_match else 'æœªæ‰¾åˆ°'}")
                    verified = ip_match and ip_match.group(0) == current_proxy['ip']
                    if verified:
                        print(f"âœ… ä»£ç†å·¥ä½œæ­£å¸¸ï¼IPåŒ¹é…: {current_proxy['ip']}")
                    else:
                        print("âŒ ä»£ç†å¤±è´¥æˆ–IPä¸åŒ¹é…ï¼")
            else:
                print(f"è¯·æ±‚å¤±è´¥: {result.error_message}")

async def demo_proxy_rotation_batch():
    """
    æ‰¹é‡å¤„ç†ä»£ç†è½®æ¢æ¼”ç¤º
    =======================================
    æ¼”ç¤ºä½¿ç”¨arun_manyå’Œå†…å­˜è°ƒåº¦å™¨çš„ä»£ç†è½®æ¢ã€‚
    """
    print("\n=== æ‰¹é‡ä»£ç†è½®æ¢æ¼”ç¤º ===")
    
    try:
        # åŠ è½½ä»£ç†å¹¶åˆ›å»ºè½®æ¢ç­–ç•¥
        proxies = load_proxies_from_env()
        if not proxies:
            print("ç¯å¢ƒä¸­æœªæ‰¾åˆ°ä»£ç†ã€‚è®¾ç½®PROXIESç¯å¢ƒå˜é‡ï¼")
            return
            
        proxy_strategy = RoundRobinProxyStrategy(proxies)
        
        # é…ç½®
        browser_config = BrowserConfig(headless=True, verbose=False)
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            proxy_rotation_strategy=proxy_strategy,
            markdown_generator=DefaultMarkdownGenerator()
        )

        # æµ‹è¯•URL - å¤šä¸ªè¯·æ±‚ä»¥æµ‹è¯•è½®æ¢
        urls = ["https://httpbin.org/ip"] * (len(proxies) * 2)  # æ¯ä¸ªä»£ç†æµ‹è¯•ä¸¤æ¬¡

        print("\nğŸ“ˆ ä½¿ç”¨ä»£ç†è½®æ¢åˆå§‹åŒ–çˆ¬è™«...")
        async with AsyncWebCrawler(config=browser_config) as crawler:
            monitor = CrawlerMonitor(
                max_visible_rows=10,
                display_mode=DisplayMode.DETAILED
            )
            
            dispatcher = MemoryAdaptiveDispatcher(
                memory_threshold_percent=80.0,
                check_interval=0.5,
                max_session_permit=1, #len(proxies),  # å¹¶å‘ä¼šè¯æ•°ä¸ä»£ç†æ•°åŒ¹é…
                # monitor=monitor
            )
            
            print("\nğŸš€ å¼€å§‹ä½¿ç”¨ä»£ç†è½®æ¢è¿›è¡Œæ‰¹é‡çˆ¬å–...")
            results = await crawler.arun_many(
                urls=urls,
                config=run_config,
                dispatcher=dispatcher
            )

            # éªŒè¯ç»“æœ
            success_count = 0
            for result in results:
                if result.success:
                    ip_match = re.search(r'(?:[0-9]{1,3}\.){3}[0-9]{1,3}', result.html)
                    current_proxy = run_config.proxy_config if run_config.proxy_config else None
                    
                    if current_proxy and ip_match:
                        print(f"URL {result.url}")
                        print(f"ä»£ç† {current_proxy['server']} -> å“åº”IP: {ip_match.group(0)}")
                        verified = ip_match.group(0) == current_proxy['ip']
                        if verified:
                            print(f"âœ… ä»£ç†å·¥ä½œæ­£å¸¸ï¼IPåŒ¹é…: {current_proxy['ip']}")
                            success_count += 1
                        else:
                            print("âŒ ä»£ç†å¤±è´¥æˆ–IPä¸åŒ¹é…ï¼")
                    print("---")
                    
            print(f"\nâœ… å®Œæˆäº†{len(results)}ä¸ªè¯·æ±‚ï¼Œå…¶ä¸­{success_count}ä¸ªä»£ç†éªŒè¯æˆåŠŸ")
            
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡ä»£ç†è½®æ¢æ¼”ç¤ºå‡ºé”™: {str(e)}")

if __name__ == "__main__":
    import asyncio
    from crawl4ai import (
        CrawlerMonitor, 
        DisplayMode,
        MemoryAdaptiveDispatcher,
        DefaultMarkdownGenerator
    )
    
    async def run_demos():
        # await demo_proxy_rotation()  # åŸå§‹å•è¯·æ±‚æ¼”ç¤º
        await demo_proxy_rotation_batch()  # æ–°çš„æ‰¹å¤„ç†æ¼”ç¤º
        
    asyncio.run(run_demos())