# Crawl4AI åŸºç¡€è®¾ç½®å’Œè®¤è¯†





github åœ°å€ï¼šhttps://github.com/unclecode/crawl4ai



## Crawl4AI å…¥é—¨æŒ‡å—

æœ¬å¿«é€Ÿå…¥é—¨æŒ‡å—ä»‹ç»äº† Crawl4AIï¼Œæ¶µç›–äº†åŸºæœ¬ç”¨æ³•ã€å…ˆè¿›åŠŸèƒ½ï¼ˆä¾‹å¦‚åˆ†å—å’Œæå–ç­–ç•¥ï¼‰ä»¥åŠå¼‚æ­¥ç¼–ç¨‹ã€‚ç”¨æˆ·å°†å­¦ä¹ å¦‚ä½•å®ç°å„ç§çˆ¬è™«æŠ€æœ¯ï¼ŒåŒ…æ‹¬æˆªå›¾ã€JSON æå–å’ŒåŠ¨æ€å†…å®¹çˆ¬å–ã€‚

## 1. ä»€ä¹ˆæ˜¯ Crawl4AIï¼Ÿ

Crawl4AI æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼‚æ­¥ç½‘ç»œçˆ¬è™«åº“ï¼Œæ—¨åœ¨ç®€åŒ–ä¿¡æ¯æ”¶é›†è¿‡ç¨‹ã€‚å®ƒå…è®¸å¼€å‘è€…å¿«é€Ÿã€æœ‰æ•ˆåœ°ä»ç½‘ç«™ä¸Šæå–æ•°æ®ï¼Œå¹¶æ”¯æŒå¤šç§æå–ç­–ç•¥å’ŒåŠ¨æ€å†…å®¹çš„å¤„ç†ã€‚é€šè¿‡ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹ï¼ŒCrawl4AI èƒ½å¤Ÿåœ¨è¿›è¡Œçˆ¬å–æ—¶æé«˜æ•ˆç‡ï¼Œä½¿å…¶åœ¨å¤„ç†å¤§é‡è¯·æ±‚æ—¶è¡¨ç°æ›´ä½³ã€‚

## 2. å®‰è£…å’Œç¯å¢ƒå‡†å¤‡

ä½¿ç”¨ Crawl4AI ä¹‹å‰ï¼Œç”¨æˆ·éœ€è¦ç¡®ä¿å®‰è£…äº†å¿…è¦çš„ Python ç¯å¢ƒå’Œä¾èµ–é¡¹ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£… Crawl4AIï¼š

```
pip install crawl4ai
```

## 3. åŸºæœ¬ç”¨æ³•

### 3.1 å¯¼å…¥æ¨¡å—å’Œåˆ›å»ºçˆ¬è™«å®ä¾‹

ç”¨æˆ·é¦–å…ˆéœ€è¦å¯¼å…¥å¿…è¦çš„æ¨¡å—å¹¶åˆ›å»º `AsyncWebCrawler` çš„å®ä¾‹ã€‚ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯ä»¥è‡ªåŠ¨å¤„ç†çˆ¬è™«çš„å¯åŠ¨å’Œå…³é—­ã€‚

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        # åœ¨è¿™é‡Œæ·»åŠ çˆ¬è™«ä»£ç 
        pass

if __name__ == "__main__":
    asyncio.run(main())
```

### 3.2 ç®€å•çš„çˆ¬è™«æ“ä½œ

ç”¨æˆ·åªéœ€æä¾›ä¸€ä¸ª URLï¼ŒCrawl4AI å°±ä¼šæ‰§è¡Œå…¶é­”æ³•ï¼

```python
async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url="https://www.nbcnews.com/business")
        print(f"åŸºæœ¬çˆ¬å–ç»“æœ: {result.markdown[:500]}")  # æ‰“å°å‰500ä¸ªå­—ç¬¦

if __name__ == "__main__":
    asyncio.run(main())
```

## 4. æˆªå›¾åŠŸèƒ½ ğŸ“¸

ç”¨æˆ·è¿˜å¯ä»¥ä½¿ç”¨ Crawl4AI è¿›è¡Œç½‘é¡µæˆªå›¾ã€‚

```
import base64

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url="https://www.nbcnews.com/business", screenshot=True)
        with open("screenshot.png", "wb") as f:
            f.write(base64.b64decode(result.screenshot))
        print("æˆªå›¾å·²ä¿å­˜ä¸º 'screenshot.png'!")

asyncio.run(main())
```

## 5. ç†è§£å‚æ•° ğŸ§ 

Crawl4AI é»˜è®¤ä¼šç¼“å­˜çˆ¬å–ç»“æœï¼Œè¿™æ„å‘³ç€å¯¹åŒä¸€ URL çš„åç»­çˆ¬å–ä¼šæ›´å¿«ã€‚ä»¥ä¸‹æ˜¯å®ç°è¿™ä¸€åŠŸèƒ½çš„ç¤ºä¾‹ã€‚

```
async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        # ç¬¬ä¸€æ¬¡çˆ¬å–ï¼ˆç¼“å­˜ç»“æœï¼‰
        result1 = await crawler.arun(url="https://www.nbcnews.com/business")
        print(f"ç¬¬ä¸€æ¬¡çˆ¬å–ç»“æœ: {result1.markdown[:100]}...")

        # å¼ºåˆ¶å†æ¬¡çˆ¬å–
        result2 = await crawler.arun(url="https://www.nbcnews.com/business", bypass_cache=True)
        print(f"ç¬¬äºŒæ¬¡çˆ¬å–ç»“æœ: {result2.markdown[:100]}...")

asyncio.run(main())
```

## 6. æ·»åŠ åˆ†å—ç­–ç•¥ ğŸ§©

ç”¨æˆ·å¯ä»¥æ·»åŠ åˆ†å—ç­–ç•¥ï¼Œä¾‹å¦‚ `RegexChunking`ï¼Œæ­¤ç­–ç•¥åŸºäºç»™å®šçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åˆ†å‰²æ–‡æœ¬ã€‚

```
from crawl4ai.chunking_strategy import RegexChunking

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            chunking_strategy=RegexChunking(patterns=["\n\n"])
        )
        print(f"RegexChunkingç»“æœ: {result.extracted_content[:200]}...")

asyncio.run(main())
```

## 7. æ·»åŠ æå–ç­–ç•¥ ğŸ§ 

ç”¨æˆ·å¯ä»¥ä½¿ç”¨æå–ç­–ç•¥ï¼Œå¦‚ `JsonCssExtractionStrategy`ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨ CSS é€‰æ‹©å™¨ä» HTML ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
        "name": "æ–°é—»æ–‡ç« ",
        "baseSelector": "article.tease-card",
        "fields": [
            {
                "name": "title",
                "selector": "h2",
                "type": "text",
            },
            {
                "name": "summary",
                "selector": "div.tease-card__info",
                "type": "text",
            }
        ],
    }

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            extraction_strategy=JsonCssExtractionStrategy(schema, verbose=True)
        )
        extracted_data = json.loads(result.extracted_content)
        print(f"æå–åˆ° {len(extracted_data)} ç¯‡æ–‡ç« ")
        print(json.dumps(extracted_data[0], indent=2))

asyncio.run(main())
```





## 6 å’Œ 7 çš„åŒºåˆ«



###  ä¸€ã€æ ¸å¿ƒç›®çš„ä¸åŒ

| ç­–ç•¥ç±»å‹                                    | ç›®çš„                                                         |
| ------------------------------------------- | ------------------------------------------------------------ |
| **`JsonCssExtractionStrategy`ï¼ˆæå–ç­–ç•¥ï¼‰** | **ä» HTML ä¸­ç»“æ„åŒ–åœ°æŠ½å–ç‰¹å®šå­—æ®µ**ï¼ˆå¦‚æ ‡é¢˜ã€æ‘˜è¦ï¼‰ï¼Œè¾“å‡ºæ˜¯ **ç»“æ„åŒ–æ•°æ®**ï¼ˆå¦‚ JSON åˆ—è¡¨ï¼‰ã€‚ |
| **`RegexChunking`ï¼ˆåˆ†å—ç­–ç•¥ï¼‰**             | **å°†åŸå§‹æ–‡æœ¬æŒ‰è§„åˆ™åˆ‡åˆ†æˆç‰‡æ®µ**ï¼ˆchunksï¼‰ï¼Œè¾“å‡ºä»æ˜¯ **éç»“æ„åŒ–çš„æ–‡æœ¬å—åˆ—è¡¨**ï¼Œç”¨äºåç»­å¤„ç†ï¼ˆå¦‚å‘é‡åŒ–ã€æ‘˜è¦ã€RAGï¼‰ã€‚ |





> âœ… æå– = â€œæˆ‘è¦æ–°é—»çš„æ ‡é¢˜å’Œæ‘˜è¦â€
>  âœ… åˆ†å— = â€œæŠŠæ•´ç¯‡æ–‡ç« æŒ‰æ®µè½åˆ‡å¼€ï¼Œæ–¹ä¾¿åé¢åˆ†æâ€



### äºŒã€å¤„ç†é˜¶æ®µä¸åŒ

- **æå–ç­–ç•¥**ï¼šåœ¨ **HTML è§£æé˜¶æ®µ** å·¥ä½œï¼Œç›´æ¥æ“ä½œ DOM æ ‘ï¼Œä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½å…ƒç´ ã€‚
- **åˆ†å—ç­–ç•¥**ï¼šåœ¨ **æ–‡æœ¬æ¸…ç†ä¹‹åã€è¿”å›ç»“æœä¹‹å‰** å·¥ä½œï¼Œæ“ä½œçš„æ˜¯çº¯æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯ `crawler` æå–çš„ `cleaned_html` æˆ– `markdown` å†…å®¹ï¼‰ã€‚





æµç¨‹ç¤ºæ„ï¼š



```tex
1HTML â†’ (å¯é€‰ï¼šJS æ¸²æŸ“) â†’ æ¸…ç†æ–‡æœ¬ï¼ˆå»å™ªã€è½¬ Markdownï¼‰ â†’ [åˆ†å—ç­–ç•¥] â†’ è¿”å› chunks
2                             â†“
3                      [æå–ç­–ç•¥] â†’ è¿”å›ç»“æ„åŒ– JSON
```





âš ï¸ æ³¨æ„ï¼š**æå–ç­–ç•¥å’Œåˆ†å—ç­–ç•¥é€šå¸¸ä¸ä¼šåŒæ—¶ç”Ÿæ•ˆ**ã€‚å¦‚æœä½ æŒ‡å®šäº† `extraction_strategy`ï¼Œ`chunking_strategy` ä¸€èˆ¬ä¼šè¢«å¿½ç•¥ï¼ˆå–å†³äº `crawl4ai` çš„å®ç°é€»è¾‘ï¼‰ã€‚ä¸¤è€…æ˜¯äº’æ–¥çš„ä½¿ç”¨è·¯å¾„ã€‚

###  ä¸‰ã€è¾“å‡ºæ ¼å¼å¯¹æ¯”

#### ä½¿ç”¨ `JsonCssExtractionStrategy`ï¼š





```json
1[
2  {
3    "title": "Trump appeals to the Supreme Court...",
4    "summary": "A federal court ruled his tariffs illegal..."
5  },
6  ...
7]
```





â†’ å¯ç›´æ¥ç”¨äºæ•°æ®åˆ†æã€æ•°æ®åº“å…¥åº“ç­‰ã€‚

#### ä½¿ç”¨ `RegexChunking(patterns=["\n\n"])`ï¼š





```python
1[
2  "Trump appeals to the Supreme Court to preserve his sweeping tariffs\n\nChris Ratcliffe / Bloomberg via Getty Images file",
3  "Apple has survived Trump's tariffs so far. It might raise iPhone prices anyway.\n\nLATEST BUSINESS NEWS",
4  ...
5]
```







â†’ æ¯ä¸ª chunk æ˜¯ä¸€æ®µè¿ç»­æ–‡æœ¬ï¼Œä¿ç•™åŸå§‹è¯­è¨€ï¼Œé€‚åˆé€å…¥ LLM åšæ‘˜è¦ã€é—®ç­”æˆ–åµŒå…¥ã€‚

------

### ğŸ›  å››ã€é€‚ç”¨åœºæ™¯ä¸åŒ

| åœºæ™¯                                     | æ¨èç­–ç•¥                             |
| ---------------------------------------- | ------------------------------------ |
| æ„å»ºæ–°é—»èšåˆ APIï¼Œéœ€è¦æ ‡é¢˜+æ‘˜è¦          | âœ… `JsonCssExtractionStrategy`        |
| å°†ç½‘é¡µå†…å®¹åˆ‡ç‰‡åå­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆRAGï¼‰    | âœ… `RegexChunking` æˆ– `TokenChunking` |
| æŠ“å–å•†å“ä»·æ ¼ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ç­‰ç»“æ„åŒ–å­—æ®µ | âœ… æå–ç­–ç•¥                           |
| å¯¹é•¿æ–‡ç« åšè¯­ä¹‰åˆ†æ®µä»¥ä¾¿ LLM ç†è§£          | âœ… åˆ†å—ç­–ç•¥                           |

------

### ğŸŒ° ä¸¾ä¸ªå®é™…ä¾‹å­ï¼ˆåŸºäºä½ æä¾›çš„ NBC é¡µé¢ï¼‰

- **ç”¨æå–ç­–ç•¥**ï¼šä½ æƒ³å¾—åˆ°â€œæ¯ç¯‡æ–°é—»çš„æ ‡é¢˜â€ï¼Œä½†é¡µé¢æ²¡æœ‰æ¸…æ™°çš„ `<article>` ç»“æ„ â†’ **å¯èƒ½å¤±è´¥æˆ–ä¸ºç©º**ã€‚
- **ç”¨åˆ†å—ç­–ç•¥**ï¼šä½ ä¸åœ¨ä¹ç»“æ„ï¼Œåªæƒ³æŠŠé¡µé¢æ‰€æœ‰æ–‡å­—æŒ‰ç©ºè¡Œåˆ‡å¼€ â†’ **èƒ½æ‹¿åˆ°ä¸€å †æ–‡æœ¬æ®µè½**ï¼Œå³ä½¿æ··æ‚äº†å›¾ç‰‡è¯´æ˜ã€é‡å¤æ ‡ç­¾ï¼Œä¹Ÿèƒ½ä½œä¸ºåŸå§‹ç´ æè¿›ä¸€æ­¥æ¸…æ´—ã€‚

------

### âœ… æ€»ç»“ï¼šå…³é”®åŒºåˆ«ä¸€è§ˆ

| ç»´åº¦         | æå–ç­–ç•¥ (`ExtractionStrategy`) | åˆ†å—ç­–ç•¥ (`ChunkingStrategy`) |
| ------------ | ------------------------------- | ----------------------------- |
| **ç›®æ ‡**     | æŠ½å–ç»“æ„åŒ–å­—æ®µ                  | åˆ‡åˆ†åŸå§‹æ–‡æœ¬                  |
| **è¾“å…¥**     | HTML DOM                        | æ¸…ç†åçš„çº¯æ–‡æœ¬                |
| **è¾“å‡º**     | JSON / å­—å…¸åˆ—è¡¨                 | å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆchunksï¼‰          |
| **ä¾èµ–**     | HTML ç»“æ„ç¨³å®šæ€§                 | æ–‡æœ¬åˆ†éš”æ¨¡å¼ï¼ˆå¦‚ `\n\n`ï¼‰     |
| **å…¸å‹ç”¨é€”** | æ•°æ®é‡‡é›†ã€çˆ¬è™«å…¥åº“              | RAGã€LLM ä¸Šä¸‹æ–‡å‡†å¤‡           |
| **æ˜¯å¦äº’æ–¥** | é€šå¸¸ä¸åˆ†å—ç­–ç•¥äº’æ–¥              | é€šå¸¸ä¸æå–ç­–ç•¥äº’æ–¥            |

------

### ğŸ’¡ å»ºè®®

- å¦‚æœä½ çŸ¥é“ç›®æ ‡ç½‘ç«™æœ‰ **ç¨³å®šç»“æ„**ï¼ˆå¦‚ç”µå•†äº§å“é¡µã€åšå®¢åˆ—è¡¨ï¼‰ï¼Œä¼˜å…ˆç”¨ **æå–ç­–ç•¥**ã€‚
- å¦‚æœç½‘ç«™ç»“æ„æ··ä¹±ã€åŠ¨æ€æ¸²æŸ“å¼ºï¼ˆå¦‚æ–°é—»é—¨æˆ·ã€è®ºå›ï¼‰ï¼Œæˆ–ä½ éœ€è¦ **å…¨æ–‡è¯­ä¹‰å¤„ç†**ï¼Œä¼˜å…ˆç”¨ **åˆ†å—ç­–ç•¥ + LLM åå¤„ç†**ã€‚

å¦‚æœä½ å¸Œæœ›å…ˆåˆ†å—å†å¯¹æ¯ä¸ª chunk åšç»“æ„åŒ–æå–ï¼Œå¯èƒ½éœ€è¦ **åˆ†ä¸¤æ­¥**ï¼šå…ˆç”¨ `chunking_strategy` è·å–æ–‡æœ¬å—ï¼Œå†å¯¹æ¯ä¸ª chunk å•ç‹¬è°ƒç”¨ LLM æå–ä¿¡æ¯ã€‚







## 8. ä½¿ç”¨ LLM æå–ç­–ç•¥ ğŸ¤–



LLMExtractionStrategy ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»ç½‘é¡µä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚

```python
from crawl4ai.extraction_strategy import LLMExtractionStrategy
import os
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="OpenAIæ¨¡å‹åç§°ã€‚")
    input_fee: str = Field(..., description="OpenAIæ¨¡å‹è¾“å…¥tokençš„è´¹ç”¨ã€‚")
    output_fee: str = Field(..., description="OpenAIæ¨¡å‹è¾“å‡ºtokençš„è´¹ç”¨ã€‚")

async def main():
    if not os.getenv("OPENAI_API_KEY"):
        print("æœªæ‰¾åˆ°OpenAI APIå¯†é’¥ã€‚è·³è¿‡æ­¤ç¤ºä¾‹ã€‚")
        return

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/",
            word_count_threshold=1,
            extraction_strategy=LLMExtractionStrategy(
                provider="openai/gpt-4o",
                api_token=os.getenv("OPENAI_API_KEY"),
                schema=OpenAIModelFee.schema(),
                extraction_type="schema",
                instruction="""ä»çˆ¬å–å†…å®¹ä¸­æå–æ‰€æœ‰æåˆ°çš„æ¨¡å‹åç§°ä»¥åŠå…¶è¾“å…¥å’Œè¾“å‡ºtokençš„è´¹ç”¨ã€‚ 
                ä¸è¦é—æ¼å†…å®¹ä¸­çš„ä»»ä½•æ¨¡å‹ã€‚æå–çš„æ¨¡å‹JSONæ ¼å¼åº”å¦‚ä¸‹æ‰€ç¤º: 
                {"model_name": "GPT-4", "input_fee": "US$10.00 / 1M tokens", "output_fee": "US$30.00 / 1M tokens"}ã€‚""",
            ),
            bypass_cache=True,
        )
        print(result.extracted_content)

asyncio.run(main())
```

## 9. äº¤äº’å¼æå– ğŸ–±ï¸

ç”¨æˆ·å¯ä»¥ä½¿ç”¨ JavaScript ä¸é¡µé¢è¿›è¡Œäº¤äº’ï¼Œç„¶åå†è¿›è¡Œæå–ã€‚

```python
async def main():
    js_code = """
    const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More'));
    loadMoreButton && loadMoreButton.click();
    """

    wait_for = """() => {
        return Array.from(document.querySelectorAll('article.tease-card')).length > 10;
    }"""

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=js_code,
            wait_for=wait_for,
            css_selector="article.tease-card",
            bypass_cache=True,
        )
        print(f"JavaScriptäº¤äº’ç»“æœ: {result.extracted_content[:500]}")

asyncio.run(main())
```

## 10. é«˜çº§ä¼šè¯çˆ¬å–ä¸åŠ¨æ€å†…å®¹ ğŸ”„

åœ¨ç°ä»£ Web åº”ç”¨ç¨‹åºä¸­ï¼Œå†…å®¹é€šå¸¸åœ¨ä¸æ›´æ”¹ URL çš„æƒ…å†µä¸‹åŠ¨æ€åŠ è½½ã€‚è¿™åœ¨å•é¡µé¢åº”ç”¨ç¨‹åºï¼ˆSPAï¼‰æˆ–ä½¿ç”¨æ— é™æ»šåŠ¨çš„ç½‘ç«™ä¸­å¾ˆå¸¸è§ã€‚ä¼ ç»Ÿçš„ä¾èµ– URL å˜åŒ–çš„çˆ¬å–æ–¹æ³•æ— æ³•åœ¨è¿™é‡Œå·¥ä½œã€‚Crawl4AI çš„é«˜çº§ä¼šè¯çˆ¬å–æŠ€æœ¯éå¸¸æœ‰ç”¨ã€‚

### 10.1 ä¼šè¯ä¿æŒ

é€šè¿‡ä½¿ç”¨ `session_id`ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä¸é¡µé¢å¤šæ¬¡äº¤äº’çš„è¿‡ç¨‹ä¸­ä¿æŒçˆ¬è™«ä¼šè¯çš„çŠ¶æ€ã€‚è¿™å¯¹äºå¯¼èˆªåŠ¨æ€åŠ è½½çš„å†…å®¹è‡³å…³é‡è¦ã€‚

### 10.2 å¼‚æ­¥ JavaScript æ‰§è¡Œ

ç”¨æˆ·å¯ä»¥æ‰§è¡Œè‡ªå®šä¹‰ JavaScriptï¼Œä»¥è§¦å‘å†…å®¹åŠ è½½æˆ–å¯¼èˆªã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œçˆ¬å– GitHub ä»“åº“ä¸­çš„å¤šä¸ªé¡µçš„æäº¤ã€‚

```python
import json
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        url = "https://github.com/microsoft/TypeScript/commits/main"
        session_id = "typescript_commits_session"
        all_commits = []

        js_next_page = """
        const button = document.querySelector('a[data-testid="pagination-next-button"]');
        if (button) button.click();
        """

        wait_for = """() => {
            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
            if (commits.length === 0) return false;
            const firstCommit = commits[0].textContent.trim();
            return firstCommit !== window.lastCommit;
        }"""

        schema = {
            "name": "æäº¤æå–å™¨",
            "baseSelector": "li.Box-sc-g0xbh4-0",
            "fields": [
                {
                    "name": "title",
                    "selector": "h4.markdown-title",
                    "type": "text",
                    "transform": "strip",
                },
            ],
        }
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

        for page in range(3):  # çˆ¬å–3é¡µ
            result = await crawler.arun(
                url=url,
                session_id=session_id,
                css_selector="li.Box-sc-g0xbh4-0",
                extraction_strategy=extraction_strategy,
                js_code=js_next_page if page > 0 else None,
                wait_for=wait_for if page > 0 else None,
                js_only=page > 0,
                bypass_cache=True,
                headless=False,
            )

            assert result.success, f"çˆ¬å–ç¬¬ {page + 1} é¡µå¤±è´¥"

            commits = json.loads(result.extracted_content)
            all_commits.extend(commits)

            print(f"ç¬¬ {page + 1} é¡µ: æ‰¾åˆ° {len(commits)} ä¸ªæäº¤")

        await crawler.crawler_strategy.kill_session(session_id)
        print(f"æˆåŠŸçˆ¬å– {len(all_commits)} ä¸ªæäº¤ï¼Œå…±3é¡µ")

asyncio.run(main())
```

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œç”¨æˆ·çˆ¬å– GitHub ä»“åº“ä¸­çš„å¤šä¸ªæäº¤é¡µé¢ã€‚URL åœ¨åŠ è½½æ›´å¤šæäº¤æ—¶ä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤ç”¨æˆ·ä½¿ç”¨ JavaScript å•å‡»â€œåŠ è½½æ›´å¤šâ€æŒ‰é’®ï¼Œå¹¶æŒ‡å®š `wait_for` æ¡ä»¶ä»¥ç¡®ä¿åœ¨æå–ä¹‹å‰æ–°å†…å®¹å·²å®Œå…¨åŠ è½½ã€‚è¿™ç§å¼ºå¤§çš„ç»„åˆä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾å¯¼èˆªå’Œæå–å¤æ‚çš„åŠ¨æ€åŠ è½½ Web åº”ç”¨ç¨‹åºä¸­çš„æ•°æ®ã€‚





























# å…¥é—¨ Crawl4AI

æ¬¢è¿ä½¿ç”¨ **Crawl4AI**ï¼Œä¸€ä¸ªå¼€æºçš„ LLM å‹å¥½å‹ç½‘é¡µçˆ¬è™«å’ŒæŠ“å–å·¥å…·ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å°†å­¦ä¼šï¼š

1. è¿è¡Œä½ çš„ **ç¬¬ä¸€ä¸ªçˆ¬å–ä»»åŠ¡**ï¼Œå¹¶ä½¿ç”¨æœ€å°åŒ–é…ç½®ã€‚
2. ç”Ÿæˆ **Markdown** è¾“å‡ºï¼ˆå¹¶äº†è§£å†…å®¹è¿‡æ»¤å™¨å¦‚ä½•å½±å“å…¶ç»“æœï¼‰ã€‚
3. ä½“éªŒä¸€ä¸ªç®€å•çš„ **åŸºäº CSS çš„æå–** æ–¹æ³•ã€‚
4. äº†è§£ **åŸºäº LLM çš„æå–**ï¼ˆåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹é€‰é¡¹ï¼‰ã€‚
5. çˆ¬å–ä¸€ä¸ª **åŠ¨æ€** é¡µé¢ï¼Œè¯¥é¡µé¢é€šè¿‡ JavaScript åŠ è½½å†…å®¹ã€‚

------

## 1. ä»‹ç»

Crawl4AI æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š

- ä¸€ä¸ªå¼‚æ­¥çˆ¬è™«ï¼Œ**`AsyncWebCrawler`**ã€‚
- é€šè¿‡ **`BrowserConfig`** å’Œ **`CrawlerRunConfig`** å¯é…ç½®æµè§ˆå™¨å’Œè¿è¡Œè®¾ç½®ã€‚
- é€šè¿‡ **`DefaultMarkdownGenerator`** è‡ªåŠ¨å°† HTML è½¬æ¢ä¸º Markdownï¼ˆæ”¯æŒå¯é€‰è¿‡æ»¤å™¨ï¼‰ã€‚
- å¤šç§æå–ç­–ç•¥ï¼ˆåŸºäº LLM æˆ– â€œä¼ ç»Ÿâ€ CSS/XPathï¼‰ã€‚

åœ¨æœ¬æŒ‡å—ç»“æŸæ—¶ï¼Œä½ å°†å®ŒæˆåŸºæœ¬çˆ¬å–ã€ç”Ÿæˆ Markdownã€å°è¯•ä¸¤ç§æå–ç­–ç•¥ï¼Œå¹¶çˆ¬å–ä¸€ä¸ªä½¿ç”¨â€œåŠ è½½æ›´å¤šâ€æŒ‰é’®æˆ– JavaScript æ›´æ–°çš„åŠ¨æ€é¡µé¢ã€‚

------

## 2. ä½ çš„ç¬¬ä¸€ä¸ªçˆ¬å–ä»»åŠ¡

ä¸‹é¢æ˜¯ä¸€ä¸ªæœ€å°åŒ–çš„ Python è„šæœ¬ï¼Œåˆ›å»ºäº† **`AsyncWebCrawler`**ï¼Œè·å–ç½‘é¡µå¹¶æ‰“å°å‰ 300 ä¸ªå­—ç¬¦çš„ Markdown è¾“å‡ºï¼š

```
import asynciofrom crawl4ai import AsyncWebCrawlerasync def main():    async with AsyncWebCrawler() as crawler:        result = await crawler.arun("https://example.com")        print(result.markdown[:300])  # æ‰“å°å‰ 300 ä¸ªå­—ç¬¦if __name__ == "__main__":    asyncio.run(main())
```

**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

- **`AsyncWebCrawler`** å¯åŠ¨äº†ä¸€ä¸ªæ— å¤´æµè§ˆå™¨ï¼ˆé»˜è®¤ä½¿ç”¨ Chromiumï¼‰ã€‚
- å®ƒè·å– `https://example.com`ã€‚
- Crawl4AI è‡ªåŠ¨å°† HTML è½¬æ¢ä¸º Markdownã€‚

ä½ ç°åœ¨å·²ç»æˆåŠŸè¿è¡Œäº†ä¸€ä¸ªç®€å•çš„çˆ¬å–ä»»åŠ¡ï¼

------

## 3. åŸºç¡€é…ç½®ï¼ˆè½»é‡ä»‹ç»ï¼‰

Crawl4AI çš„çˆ¬è™«å¯ä»¥é€šè¿‡ä¸¤ä¸ªä¸»è¦ç±»è¿›è¡Œé«˜åº¦è‡ªå®šä¹‰ï¼š

1. **`BrowserConfig`**ï¼šæ§åˆ¶æµè§ˆå™¨è¡Œä¸ºï¼ˆæ— å¤´æ¨¡å¼æˆ–å¸¦ UIï¼Œç”¨æˆ·ä»£ç†ï¼ŒJavaScript å¼€å…³ç­‰ï¼‰ã€‚
2. **`CrawlerRunConfig`**ï¼šæ§åˆ¶çˆ¬å–è¿è¡Œæ–¹å¼ï¼ˆç¼“å­˜ã€æå–ã€è¶…æ—¶ã€é’©å­ç­‰ï¼‰ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ä½¿ç”¨ç¤ºä¾‹ï¼š

```
import asynciofrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheModeasync def main():    browser_conf = BrowserConfig(headless=True)  # è®¾ä¸º False ä»¥è§‚å¯Ÿæµè§ˆå™¨    run_conf = CrawlerRunConfig(        cache_mode=CacheMode.BYPASS    )    async with AsyncWebCrawler(config=browser_conf) as crawler:        result = await crawler.arun(            url="https://example.com",            config=run_conf        )        print(result.markdown)if __name__ == "__main__":    asyncio.run(main())
```

> **é‡è¦**ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œç¼“å­˜æ¨¡å¼è®¾ç½®ä¸º `CacheMode.ENABLED`ã€‚å¦‚æœéœ€è¦è·å–æœ€æ–°å†…å®¹ï¼Œè¯·å°†å…¶è®¾ç½®ä¸º `CacheMode.BYPASS`ã€‚

åœ¨åç»­æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ›´é«˜çº§çš„é…ç½®ï¼ˆå¦‚å¯ç”¨ä»£ç†ã€PDF è¾“å‡ºã€å¤šæ ‡ç­¾é¡µä¼šè¯ç­‰ï¼‰ã€‚ç›®å‰ï¼Œä½ åªéœ€çŸ¥é“å¦‚ä½•ä¼ é€’è¿™äº›å¯¹è±¡æ¥ç®¡ç†çˆ¬å–ä»»åŠ¡ã€‚

------

## 4. ç”Ÿæˆ Markdown è¾“å‡º

Crawl4AI é»˜è®¤ä¼šè‡ªåŠ¨å°†æ¯ä¸ªçˆ¬å–çš„é¡µé¢è½¬æ¢ä¸º Markdownã€‚ä½†å…·ä½“çš„è¾“å‡ºå–å†³äºä½ æ˜¯å¦æŒ‡å®šäº† **Markdown ç”Ÿæˆå™¨** æˆ– **å†…å®¹è¿‡æ»¤å™¨**ã€‚

- **`result.markdown`**ï¼š ç›´æ¥çš„ HTML è½¬ Markdown è½¬æ¢ç»“æœã€‚
- **`result.markdown.fit_markdown`**ï¼š åº”ç”¨äº†ä»»ä½•å·²é…ç½® **å†…å®¹è¿‡æ»¤å™¨**ï¼ˆå¦‚ `PruningContentFilter`ï¼‰åçš„ Markdownã€‚

### ç¤ºä¾‹ï¼šä½¿ç”¨ `DefaultMarkdownGenerator` è¿›è¡Œè¿‡æ»¤

```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfigfrom crawl4ai.content_filter_strategy import PruningContentFilterfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGeneratormd_generator = DefaultMarkdownGenerator(    content_filter=PruningContentFilter(threshold=0.4, threshold_type="fixed"))config = CrawlerRunConfig(    cache_mode=CacheMode.BYPASS,    markdown_generator=md_generator)async with AsyncWebCrawler() as crawler:    result = await crawler.arun("https://news.ycombinator.com", config=config)    print("åŸå§‹ Markdown é•¿åº¦:", len(result.markdown.raw_markdown))    print("è¿‡æ»¤å Markdown é•¿åº¦:", len(result.markdown.fit_markdown))
```

**æ³¨æ„**ï¼šå¦‚æœ **ä¸** æŒ‡å®šå†…å®¹è¿‡æ»¤å™¨æˆ– Markdown ç”Ÿæˆå™¨ï¼Œä½ é€šå¸¸åªèƒ½çœ‹åˆ°åŸå§‹ Markdownã€‚`PruningContentFilter` å¯èƒ½ä¼šå¢åŠ  `50ms` å¤„ç†æ—¶é—´ã€‚

------

## 5. ç®€å•æ•°æ®æå–ï¼ˆåŸºäº CSSï¼‰

Crawl4AI å…è®¸ä½¿ç”¨ CSS æˆ– XPath é€‰æ‹©å™¨æå–ç»“æ„åŒ–æ•°æ®ï¼ˆJSONï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäº CSS çš„æœ€å°ç¤ºä¾‹ï¼š

```
import asyncioimport jsonfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheModefrom crawl4ai.extraction_strategy import JsonCssExtractionStrategyasync def main():    schema = {        "name": "Example Items",        "baseSelector": "div.item",        "fields": [            {"name": "title", "selector": "h2", "type": "text"},            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}        ]    }    raw_html = "<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>"    async with AsyncWebCrawler() as crawler:        result = await crawler.arun(            url="raw://" + raw_html,            config=CrawlerRunConfig(                cache_mode=CacheMode.BYPASS,                extraction_strategy=JsonCssExtractionStrategy(schema)            )        )        data = json.loads(result.extracted_content)        print(data)if __name__ == "__main__":    asyncio.run(main())
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ï¼Ÿ**

- é€‚ç”¨äºé‡å¤çš„é¡µé¢ç»“æ„ï¼ˆå¦‚å•†å“åˆ—è¡¨ã€æ–‡ç« ç­‰ï¼‰ã€‚
- æ— éœ€ AIï¼ŒèŠ‚çœ API æˆæœ¬ã€‚
- çˆ¬è™«è¿”å› JSON å­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿è§£ææˆ–å­˜å‚¨ã€‚