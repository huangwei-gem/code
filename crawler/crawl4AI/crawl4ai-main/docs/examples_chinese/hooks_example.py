"""
é’©å­å‡½æ•°ç¤ºä¾‹
æ¼”ç¤º Crawl4AI ä¸­ä¸åŒç±»å‹çš„é’©å­å‡½æ•°ä½¿ç”¨æ–¹æ³•
"""

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext


async def main():
    print("ğŸ”— é’©å­ç¤ºä¾‹ï¼šæ¼”ç¤ºä¸åŒé’©å­çš„ä½¿ç”¨åœºæ™¯")

    # é…ç½®æµè§ˆå™¨è®¾ç½®
    browser_config = BrowserConfig(headless=True)

    # é…ç½®çˆ¬è™«è®¾ç½®
    crawler_run_config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",  # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
        wait_for="body",  # ç­‰å¾… body å…ƒç´ åŠ è½½
        cache_mode=CacheMode.BYPASS,  # ç»•è¿‡ç¼“å­˜
    )

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AsyncWebCrawler(config=browser_config)

    # å®šä¹‰å’Œè®¾ç½®é’©å­å‡½æ•°
    async def on_browser_created(browser, context: BrowserContext, **kwargs):
        """æµè§ˆå™¨åˆ›å»ºåè°ƒç”¨çš„é’©å­"""
        print("[é’©å­] on_browser_created - æµè§ˆå™¨å‡†å¤‡å°±ç»ªï¼")
        # ç¤ºä¾‹ï¼šä¸ºæ‰€æœ‰è¯·æ±‚è®¾ç½® cookie
        return browser

    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        """æ–°é¡µé¢å’Œä¸Šä¸‹æ–‡åˆ›å»ºåè°ƒç”¨çš„é’©å­"""
        print("[é’©å­] on_page_context_created - æ–°é¡µé¢å·²åˆ›å»ºï¼")
        # ç¤ºä¾‹ï¼šè®¾ç½®é»˜è®¤è§†å£å¤§å°
        await context.add_cookies(
            [
                {
                    "name": "session_id",
                    "value": "example_session",
                    "domain": ".example.com",
                    "path": "/",
                }
            ]
        )
        await page.set_viewport_size({"width": 1080, "height": 800})
        return page

    async def on_user_agent_updated(
        page: Page, context: BrowserContext, user_agent: str, **kwargs
    ):
        """ç”¨æˆ·ä»£ç†æ›´æ–°æ—¶è°ƒç”¨çš„é’©å­"""
        print(f"[é’©å­] on_user_agent_updated - æ–°ç”¨æˆ·ä»£ç†: {user_agent}")
        return page

    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):
        """è‡ªå®šä¹‰ JavaScript æ‰§è¡Œåè°ƒç”¨çš„é’©å­"""
        print("[é’©å­] on_execution_started - è‡ªå®šä¹‰ JavaScript å·²æ‰§è¡Œï¼")
        return page

    async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):
        """å¯¼èˆªåˆ°æ¯ä¸ª URL ä¹‹å‰è°ƒç”¨çš„é’©å­"""
        print(f"[é’©å­] before_goto - å³å°†è®¿é—®: {url}")
        # ç¤ºä¾‹ï¼šä¸ºè¯·æ±‚æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
        await page.set_extra_http_headers({"Custom-Header": "my-value"})
        return page

    async def after_goto(
        page: Page, context: BrowserContext, url: str, response: dict, **kwargs
    ):
        """å¯¼èˆªåˆ°æ¯ä¸ª URL ä¹‹åè°ƒç”¨çš„é’©å­"""
        print(f"[é’©å­] after_goto - æˆåŠŸåŠ è½½: {url}")
        # ç¤ºä¾‹ï¼šç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
        try:
            await page.wait_for_selector(".content", timeout=1000)
            print("å†…å®¹å…ƒç´ å·²æ‰¾åˆ°ï¼")
        except:
            print("å†…å®¹å…ƒç´ æœªæ‰¾åˆ°ï¼Œç»§ç»­æ‰§è¡Œ")
        return page

    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):
        """è·å– HTML å†…å®¹ä¹‹å‰è°ƒç”¨çš„é’©å­"""
        print("[é’©å­] before_retrieve_html - å³å°†è·å– HTML å†…å®¹")
        # ç¤ºä¾‹ï¼šæ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘å»¶è¿ŸåŠ è½½
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        return page

    async def before_return_html(
        page: Page, context: BrowserContext, html: str, **kwargs
    ):
        """è¿”å› HTML å†…å®¹ä¹‹å‰è°ƒç”¨çš„é’©å­"""
        print(f"[é’©å­] before_return_html - è·å–åˆ° HTML å†…å®¹ (é•¿åº¦: {len(html)})")
        # ç¤ºä¾‹ï¼šå¯ä»¥åœ¨æ­¤å¤„ä¿®æ”¹ HTML å†…å®¹
        return page

    # è®¾ç½®æ‰€æœ‰é’©å­
    crawler.crawler_strategy.set_hook("on_browser_created", on_browser_created)
    crawler.crawler_strategy.set_hook(
        "on_page_context_created", on_page_context_created
    )
    crawler.crawler_strategy.set_hook("on_user_agent_updated", on_user_agent_updated)
    crawler.crawler_strategy.set_hook("on_execution_started", on_execution_started)
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)
    crawler.crawler_strategy.set_hook("before_retrieve_html", before_retrieve_html)
    crawler.crawler_strategy.set_hook("before_return_html", before_return_html)

    await crawler.start()

    # ç¤ºä¾‹ï¼šçˆ¬å–ç®€å•ç½‘ç«™
    url = "https://example.com"
    result = await crawler.arun(url, config=crawler_run_config)
    print(f"\nçˆ¬å– URL: {result.url}")
    print(f"HTML é•¿åº¦: {len(result.html)}")
    print(f"çˆ¬å–æˆåŠŸ: {result.success}")

    await crawler.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())