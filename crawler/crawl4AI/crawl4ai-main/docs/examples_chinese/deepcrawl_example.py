import asyncio
import time

from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter,
    ContentRelevanceFilter,
    SEOFilter,
)
from crawl4ai.deep_crawling.scorers import (
    KeywordRelevanceScorer,
)


# 1ï¸âƒ£ åŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½®
async def basic_deep_crawl():
    """
    ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½® - æ¼”ç¤ºç®€å•çš„ä¸¤çº§æ·±åº¦çˆ¬å–ã€‚

    æœ¬å‡½æ•°å±•ç¤ºï¼š
    - å¦‚ä½•è®¾ç½®BFSDeepCrawlStrategyï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼‰
    - è®¾ç½®æ·±åº¦å’ŒåŸŸåå‚æ•°
    - å¤„ç†ç»“æœä»¥æ˜¾ç¤ºå±‚æ¬¡ç»“æ„
    """
    print("\n===== åŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½® =====")

    # é…ç½®ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ç­–ç•¥çš„2çº§æ·±åº¦çˆ¬å–
    # max_depth=2 è¡¨ç¤ºï¼šåˆå§‹é¡µé¢ï¼ˆæ·±åº¦0ï¼‰+ 2ä¸ªæ›´å¤šå±‚çº§
    # include_external=False è¡¨ç¤ºï¼šåªè·ŸéšåŒä¸€åŸŸåå†…çš„é“¾æ¥
    config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=2, include_external=False),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=True,  # çˆ¬å–æœŸé—´æ˜¾ç¤ºè¿›åº¦
    )

    async with AsyncWebCrawler() as crawler:
        start_time = time.perf_counter()
        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        # æŒ‰æ·±åº¦åˆ†ç»„ç»“æœä»¥å¯è§†åŒ–çˆ¬å–æ ‘
        pages_by_depth = {}
        for result in results:
            depth = result.metadata.get("depth", 0)
            if depth not in pages_by_depth:
                pages_by_depth[depth] = []
            pages_by_depth[depth].append(result.url)

        print(f"âœ… æ€»å…±çˆ¬å–äº† {len(results)} ä¸ªé¡µé¢")

        # æŒ‰æ·±åº¦æ˜¾ç¤ºçˆ¬å–ç»“æ„
        for depth, urls in sorted(pages_by_depth.items()):
            print(f"\næ·±åº¦ {depth}: {len(urls)} ä¸ªé¡µé¢")
            # æ˜¾ç¤ºæ¯ä¸ªæ·±åº¦çš„å‰3ä¸ªURLä½œä¸ºç¤ºä¾‹
            for url in urls[:3]:
                print(f"  â†’ {url}")
            if len(urls) > 3:
                print(f"  ... è¿˜æœ‰ {len(urls) - 3} ä¸ª")

        print(
            f"\nâœ… æ€§èƒ½ï¼š{len(results)} ä¸ªé¡µé¢ç”¨äº† {time.perf_counter() - start_time:.2f} ç§’"
        )

# 2ï¸âƒ£ æµå¼ä¸éæµå¼æ‰§è¡Œ
async def stream_vs_nonstream():
    """
    ç¬¬äºŒéƒ¨åˆ†ï¼šæ¼”ç¤ºæµå¼ä¸éæµå¼æ‰§è¡Œçš„åŒºåˆ«ã€‚

    éæµå¼ï¼šç­‰å¾…æ‰€æœ‰ç»“æœåå†å¤„ç†
    æµå¼ï¼šç»“æœå¯ç”¨æ—¶ç«‹å³å¤„ç†
    """
    print("\n===== æµå¼ä¸éæµå¼æ‰§è¡Œ =====")

    # ä¸¤ä¸ªç¤ºä¾‹çš„é€šç”¨é…ç½®
    base_config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1, include_external=False),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=False,
    )

    async with AsyncWebCrawler() as crawler:
        # éæµå¼æ¨¡å¼
        print("\nğŸ“Š éæµå¼æ¨¡å¼ï¼š")
        print("  åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰ç»“æœåœ¨è¿”å›å‰éƒ½ä¼šè¢«æ”¶é›†ã€‚")

        non_stream_config = base_config.clone()
        non_stream_config.stream = False

        start_time = time.perf_counter()
        results = await crawler.arun(
            url="https://docs.crawl4ai.com", config=non_stream_config
        )

        print(f"  âœ… ä¸€æ¬¡æ€§æ”¶åˆ°æ‰€æœ‰ {len(results)} ä¸ªç»“æœ")
        print(f"  âœ… æ€»è€—æ—¶ï¼š{time.perf_counter() - start_time:.2f} ç§’")

        # æµå¼æ¨¡å¼
        print("\nğŸ“Š æµå¼æ¨¡å¼ï¼š")
        print("  åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œç»“æœå¯ç”¨æ—¶ç«‹å³å¤„ç†ã€‚")

        stream_config = base_config.clone()
        stream_config.stream = True

        start_time = time.perf_counter()
        result_count = 0
        first_result_time = None

        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=stream_config
        ):
            result_count += 1
            if result_count == 1:
                first_result_time = time.perf_counter() - start_time
                print(
                    f"  âœ… ç¬¬ä¸€ä¸ªç»“æœåœ¨ {first_result_time:.2f} ç§’åæ”¶åˆ°: {result.url}"
                )
            elif result_count % 5 == 0:  # ä¸ºäº†ç®€æ´ï¼Œæ¯5ä¸ªç»“æœæ˜¾ç¤ºä¸€æ¬¡
                print(f"  â†’ ç»“æœ #{result_count}: {result.url}")

        print(f"  âœ… æ€»è®¡: {result_count} ä¸ªç»“æœ")
        print(f"  âœ… ç¬¬ä¸€ä¸ªç»“æœ: {first_result_time:.2f} ç§’")
        print(f"  âœ… æ‰€æœ‰ç»“æœ: {time.perf_counter() - start_time:.2f} ç§’")
        print("\nğŸ” å…³é”®è¦ç‚¹ï¼šæµå¼å…è®¸ç«‹å³å¤„ç†ç»“æœ")

# 3ï¸âƒ£ å¼•å…¥è¿‡æ»¤å™¨å’Œè¯„åˆ†å™¨
async def filters_and_scorers():
    """
    ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¼”ç¤ºè¿‡æ»¤å™¨å’Œè¯„åˆ†å™¨çš„ä½¿ç”¨ï¼Œå®ç°æ›´æœ‰é’ˆå¯¹æ€§çš„çˆ¬å–ã€‚

    æœ¬å‡½æ•°é€æ­¥æ·»åŠ ï¼š
    1. å•ä¸ªURLæ¨¡å¼è¿‡æ»¤å™¨
    2. é“¾ä¸­çš„å¤šä¸ªè¿‡æ»¤å™¨
    3. ç”¨äºé¡µé¢ä¼˜å…ˆçº§çš„è¯„åˆ†å™¨
    """
    print("\n===== è¿‡æ»¤å™¨å’Œè¯„åˆ†å™¨ =====")

    async with AsyncWebCrawler() as crawler:
        # å•ä¸ªè¿‡æ»¤å™¨ç¤ºä¾‹
        print("\nğŸ“Š ç¤ºä¾‹1ï¼šå•ä¸ªURLæ¨¡å¼è¿‡æ»¤å™¨")
        print("  åªçˆ¬å–URLä¸­åŒ…å«'core'çš„é¡µé¢")

        # åˆ›å»ºåªå…è®¸URLä¸­åŒ…å«'guide'çš„è¿‡æ»¤å™¨
        url_filter = URLPatternFilter(patterns=["*core*"])

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1,
                include_external=False,
                filter_chain=FilterChain([url_filter]),  # å•ä¸ªè¿‡æ»¤å™¨
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            cache_mode=CacheMode.BYPASS,
            verbose=True,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"  âœ… çˆ¬å–äº† {len(results)} ä¸ªåŒ¹é…'*core*'çš„é¡µé¢")
        for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            print(f"  â†’ {result.url}")
        if len(results) > 3:
            print(f"  ... è¿˜æœ‰ {len(results) - 3} ä¸ª")

        # å¤šä¸ªè¿‡æ»¤å™¨ç¤ºä¾‹
        print("\nğŸ“Š ç¤ºä¾‹2ï¼šé“¾ä¸­çš„å¤šä¸ªè¿‡æ»¤å™¨")
        print("  åªçˆ¬å–æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„é¡µé¢ï¼š")
        print("  1. URLä¸­åŒ…å«'2024'")
        print("  2. æ¥è‡ª'techcrunch.com'")
        print("  3. æ˜¯text/htmlæˆ–application/javascriptå†…å®¹ç±»å‹")

        # åˆ›å»ºè¿‡æ»¤å™¨é“¾
        filter_chain = FilterChain(
            [
                URLPatternFilter(patterns=["*2024*"]),
                DomainFilter(
                    allowed_domains=["techcrunch.com"],
                    blocked_domains=["guce.techcrunch.com", "oidc.techcrunch.com"],
                ),
                ContentTypeFilter(
                    allowed_types=["text/html", "application/javascript"]
                ),
            ]
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, include_external=False, filter_chain=filter_chain
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
        )

        results = await crawler.arun(url="https://techcrunch.com", config=config)

        print(f"  âœ… åº”ç”¨æ‰€æœ‰è¿‡æ»¤å™¨åçˆ¬å–äº† {len(results)} ä¸ªé¡µé¢")
        for result in results[:3]:
            print(f"  â†’ {result.url}")