#!/usr/bin/env python3
"""
Crawl4AI ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ crawl4ai è¿›è¡Œç½‘é¡µçˆ¬å–
"""

import asyncio
from crawl4ai import AsyncWebCrawler

async def simple_crawl_example():
    """ç®€å•çš„ç½‘é¡µçˆ¬å–ç¤ºä¾‹"""
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    async with AsyncWebCrawler() as crawler:
        # çˆ¬å–ä¸€ä¸ªç½‘é¡µ
        url = "https://blog.csdn.net/weixin_41477468/article/details/137524530"
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        try:
            result = await crawler.arun(
                url=url,
                bypass_cache=True  # ç»•è¿‡ç¼“å­˜
            )
            
            if result.success:
                print("âœ… çˆ¬å–æˆåŠŸ!")
                print(f"é¡µé¢URL: {result.url}")
                print(f"HTMLé•¿åº¦: {len(result.html)} å­—ç¬¦")
                if result.markdown:
                    print(f"Markdowné•¿åº¦: {len(str(result.markdown))} å­—ç¬¦")
                else:
                    print("Markdown: æ— ")
                
                # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                with open("example_result.md", "w", encoding="utf-8") as f:
                    f.write(f"# çˆ¬å–ç»“æœ\n\n")
                    f.write(f"URL: {url}\n\n")
                    if result.markdown:
                        f.write(str(result.markdown))
                
                print("ç»“æœå·²ä¿å­˜åˆ° example_result.md")
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

async def advanced_crawl_example():
    """é«˜çº§çˆ¬å–ç¤ºä¾‹ - ä½¿ç”¨é…ç½®é€‰é¡¹"""
    
    async with AsyncWebCrawler() as crawler:
        url = "https://blog.csdn.net/weixin_41477468/article/details/137524530"
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        try:
            result = await crawler.arun(
                url=url,
                bypass_cache=True,
                # ç§»é™¤å›¾ç‰‡ä»¥å‡å°‘æ•°æ®é‡
                exclude_external_images=True,
                # è®¾ç½®ç”¨æˆ·ä»£ç†
                user_agent="Mozilla/5.0 (compatible; Crawl4AI/1.0)",
                # è®¾ç½®è¶…æ—¶
                timeout=30000,  # 30ç§’
                # æå–ä¸»è¦å†…å®¹
                word_count_threshold=10,  # æœ€å°‘å­—æ•°é˜ˆå€¼
            )
            
            if result.success:
                print("âœ… é«˜çº§çˆ¬å–æˆåŠŸ!")
                print(f"é¡µé¢URL: {result.url}")
                if result.markdown:
                    print(f"Markdowné•¿åº¦: {len(str(result.markdown))} å­—ç¬¦")
                else:
                    print("Markdown: æ— ")
                print(f"æå–çš„é“¾æ¥æ•°é‡: {len(result.links) if result.links else 0}")
                
                # ä¿å­˜ç»“æœ
                with open("python_org_result.md", "w", encoding="utf-8") as f:
                    f.write(f"# é«˜çº§çˆ¬å–ç»“æœ\n\n")
                    f.write(f"URL: {url}\n\n")
                    if result.markdown:
                        f.write(str(result.markdown))
                
                print("ç»“æœå·²ä¿å­˜åˆ° python_org_result.md")
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

async def batch_crawl_example():
    """æ‰¹é‡çˆ¬å–ç¤ºä¾‹"""
    
    urls = [
        "https://example.com",
        "https://httpbin.org/html",
        "https://httpbin.org/json"
    ]
    
    async with AsyncWebCrawler() as crawler:
        print(f"æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªç½‘é¡µ...")
        
        tasks = []
        for url in urls:
            task = crawler.arun(
                url=url,
                bypass_cache=True,
                timeout=15000  # 15ç§’è¶…æ—¶
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful_count = 0
        for i, result in enumerate(results):
            url = urls[i]
            if isinstance(result, Exception):
                print(f"âŒ {url} - é”™è¯¯: {result}")
            elif result.success:
                successful_count += 1
                if result.markdown:
                    print(f"âœ… {url} - æˆåŠŸ (é•¿åº¦: {len(str(result.markdown))} å­—ç¬¦)")
                else:
                    print(f"âœ… {url} - æˆåŠŸ (æ— Markdownå†…å®¹)")
            else:
                print(f"âŒ {url} - å¤±è´¥: {result.error_message}")
        
        print(f"\næ‰¹é‡çˆ¬å–å®Œæˆ: {successful_count}/{len(urls)} æˆåŠŸ")

async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    
    print("ğŸ•·ï¸  Crawl4AI ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # ç¤ºä¾‹ 1: ç®€å•çˆ¬å–
    print("\nğŸ“‹ ç¤ºä¾‹ 1: ç®€å•çˆ¬å–")
    print("-" * 30)
    await simple_crawl_example()
    
    # ç¤ºä¾‹ 2: é«˜çº§çˆ¬å–
    print("\nğŸ“‹ ç¤ºä¾‹ 2: é«˜çº§çˆ¬å–")
    print("-" * 30)
    await advanced_crawl_example()
    
    # # ç¤ºä¾‹ 3: æ‰¹é‡çˆ¬å–
    # print("\nğŸ“‹ ç¤ºä¾‹ 3: æ‰¹é‡çˆ¬å–")
    # print("-" * 30)
    # await batch_crawl_example()
    
    print("\nâœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())