from typing import Any
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, MemoryAdaptiveDispatcher
from urllib.parse import urlparse, urldefrag
import requests
from xml.etree import ElementTree
import aiohttp


def is_sitemap(url: str) -> bool:
    return url.endswith("sitemap.xml") or "sitemap" in urlparse(url).path


def is_txt(url: str) -> bool:
    return url.endswith(".txt")

def parse_sitemap(sitemap_url: str) -> list[str]:
    resp = requests.get(sitemap_url)
    urls = []

    if resp.status_code == 200:
        try:
            tree = ElementTree.fromstring(resp.content)
            urls = [loc.text for loc in tree.findall('.//{*}loc')]
        except Exception as e:
            print(f"Error parsing sitemap XML: {e}")

    return urls


async def auto_get_sitemap_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        base = f"{parsed.scheme}://{parsed.netloc}"
        candidate_sitemaps = [
            f"{base}/sitemap.xml",
            f"{base}/sitemap_index.xml",
        ]
        found_sitemap = None
        async with aiohttp.ClientSession() as session:
            for sitemap_url in candidate_sitemaps:
                try:
                    async with session.get(sitemap_url, timeout=5) as resp:
                        if resp.status == 200:
                            text = await resp.text()
                            # 简单判断内容是否像sitemap
                            if "<urlset" in text or "<sitemapindex" in text:
                                found_sitemap = sitemap_url
                                break
                except Exception:
                    continue
        return found_sitemap
    except aiohttp.ClientConnectionError:
        pass  # 或 logging.debug(str(e))

async def crawl_markdown_file(
    crawler: AsyncWebCrawler, url: str
) -> list[dict[str, Any]]:
    crawl_config = CrawlerRunConfig()

    result = await crawler.arun(url=url, config=crawl_config)
    if result.success and result.markdown:
        return [{"url": url, "markdown": result.markdown}]
    else:
        print(f"Failed to crawl {url}: {result.error_message}")
        return []

async def crawl_batch(crawler: AsyncWebCrawler, urls: list[str], max_concurrent: int = 10) -> list[dict[str, Any]]:
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent
    )

    results = await crawler.arun_many(urls=urls, config=crawl_config, dispatcher=dispatcher)
    return [{'url': r.url, 'markdown': r.markdown} for r in results if r.success and r.markdown]


async def crawl_recursive_internal_links(crawler: AsyncWebCrawler, start_urls: list[str], max_depth: int = 3, max_concurrent: int = 10) -> list[dict[str, Any]]:
    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent
    )

    visited = set()

    def normalize_url(url):
        return urldefrag(url)[0]

    current_urls = set([normalize_url(u) for u in start_urls])
    results_all = []

    for depth in range(max_depth):
        urls_to_crawl = [normalize_url(url) for url in current_urls if normalize_url(url) not in visited]
        if not urls_to_crawl:
            break

        results = await crawler.arun_many(urls=urls_to_crawl, config=run_config, dispatcher=dispatcher)
        next_level_urls = set()

        for result in results:
            norm_url = normalize_url(result.url)
            visited.add(norm_url)

            if result.success and result.markdown:
                results_all.append({'url': result.url, 'markdown': result.markdown})
                for link in result.links.get("internal", []):
                    next_url = normalize_url(link["href"])
                    if next_url not in visited:
                        next_level_urls.add(next_url)

        current_urls = next_level_urls

    return results_all